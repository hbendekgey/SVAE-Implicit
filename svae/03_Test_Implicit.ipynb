{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c223035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "from jax import vmap, grad, random, jit, tree_map\n",
    "import jax.numpy as jnp\n",
    "from inference.MP_Inference import lds_inference, lds_inference_sequential, lds_inference_homog, hmm_inference, lds_to_hmm_mf, hmm_to_lds_mf\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from inference import SLDS_Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6758134",
   "metadata": {},
   "source": [
    "This notebook assumes `slds.pkl` has been constructed by the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d94ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"saved_models/slds.pkl\", 'rb') as f:\n",
    "    recog_potentials, pgm_potentials, _, _ = pickle.load(f)\n",
    "\n",
    "all_params = (recog_potentials, *pgm_potentials)\n",
    "\n",
    "initializer = random.PRNGKey(47)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564df3de",
   "metadata": {},
   "source": [
    "In this notebook, we will set the convergence threshold used to prematurely end iterative inference to 0, thus guaranteeing we take the specified number of inference steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd29d918",
   "metadata": {},
   "outputs": [],
   "source": [
    "SLDS_Inference.CONV_THRESH = 0.\n",
    "_, cat_expected_stats1 = SLDS_Inference.slds_inference_unrolled(*all_params, initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d1ca7c",
   "metadata": {},
   "source": [
    "Once a function gets compiled by jit, changing a global variable won't change the inner workings of the function. If we want to try running it with different global variable setting, we have to reload (ideally this parameter would be an argument, but this made less sense for model training than it does here for debugging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99d07606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(SLDS_Inference)\n",
    "SLDS_Inference.MAX_ITER = 20\n",
    "SLDS_Inference.CONV_THRESH = 0.\n",
    "\n",
    "_, cat_expected_stats2 = SLDS_Inference.slds_inference_unrolled(*all_params, initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575f69d5",
   "metadata": {},
   "source": [
    "The endpoint of optimization is slightly different after 10 vs 20 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "875a93f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.00150787, dtype=float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.mean(jnp.abs(cat_expected_stats2 - cat_expected_stats1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb851972",
   "metadata": {},
   "source": [
    "We can check how converged these two values are by how much they change after an additional block update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5411e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.00027077, dtype=float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward_iter_block(cat_expected_stats):\n",
    "    recog_potentials, E_mniw_params, init, E_init_normalizer, E_init_lps, E_trans_lps = all_params\n",
    "    \n",
    "    gaus_natparam, E_prior_logZ = hmm_to_lds_mf(cat_expected_stats, E_mniw_params, E_init_normalizer)\n",
    "    gaus_expected_stats, gaus_logZ, _ = lds_inference(recog_potentials, init, gaus_natparam)\n",
    "\n",
    "    cat_natparam = lds_to_hmm_mf(gaus_expected_stats, E_mniw_params)\n",
    "    cat_es, hmm_logZ, _ = hmm_inference(E_init_lps, E_trans_lps, cat_natparam)\n",
    "    return cat_es\n",
    "\n",
    "jnp.mean(jnp.abs(forward_iter_block(cat_expected_stats1) - cat_expected_stats1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6f246d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(9.92897613e-05, dtype=float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.mean(jnp.abs(forward_iter_block(cat_expected_stats2) - cat_expected_stats2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53ab09e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(8.96105988e-08, dtype=float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(SLDS_Inference)\n",
    "SLDS_Inference.MAX_ITER = 100\n",
    "SLDS_Inference.CONV_THRESH = 0.\n",
    "\n",
    "_, cat_expected_stats3 = SLDS_Inference.slds_inference_unrolled(*all_params, initializer)\n",
    "\n",
    "jnp.mean(jnp.abs(forward_iter_block(cat_expected_stats3) - cat_expected_stats3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a064a8",
   "metadata": {},
   "source": [
    "Due to numerical imprecisions, it may be impossible to perfectly reach a fixed point of this block update function. However, we can get extremely close.\n",
    "\n",
    "Now let's check to make sure the implicit function gives the same (forward pass) output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fa7a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaus_expected_stats, cat_expected_stats = SLDS_Inference.slds_inference_implicit(*all_params, initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e70d3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert jnp.isclose(cat_expected_stats, cat_expected_stats3).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a1578b",
   "metadata": {},
   "source": [
    "#### Gradient check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b835c552",
   "metadata": {},
   "source": [
    "Now, let's start comparing the gradients computed by the two methods. \n",
    "\n",
    "This example is so small (a single sequence with length $T=100$ and dimension $d=5$ that unrolled gradients work fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0c07952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1721.819184951477\n"
     ]
    }
   ],
   "source": [
    "# start by defining some arbitrary scalar-valued function of the output, so we can backprop\n",
    "def composition_func(params):\n",
    "    a,b,_ = SLDS_Inference.slds_inference_unrolled_baseline(*params, initializer)\n",
    "    \n",
    "    # computes KL(q(z,k) || p(z,k))\n",
    "    c = SLDS_Inference.slds_kl(*params, a, b, 0.) \n",
    "    return sum(tree_map(lambda x: x.sum(), a)) + c\n",
    "\n",
    "grad_func_unrolled = (grad(composition_func))\n",
    "unrolled_grads = grad_func_unrolled(all_params)\n",
    "print(composition_func(all_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50c85a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1721.8191849515183\n"
     ]
    }
   ],
   "source": [
    "def composition_func(params):\n",
    "    # one of the implicit implementations, inherits the forward pass from slds_inference_implicit\n",
    "    a,b = SLDS_Inference.slds_inference_cgsolve(*params, initializer) \n",
    "    \n",
    "    # computes KL(q(z,k) || p(z,k))\n",
    "    c = SLDS_Inference.slds_kl(*params, a, b, 0.) \n",
    "    return sum(tree_map(lambda x: x.sum(), a)) + c\n",
    "\n",
    "grad_func_implicit = jit(grad(composition_func))\n",
    "implicit_grads = grad_func_implicit(all_params)\n",
    "print(composition_func(all_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3fe558",
   "metadata": {},
   "source": [
    "Numerical impressions accrue, but we see below the gradients are very similar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "850397eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((Array(0.00029938, dtype=float32), Array(0.00022077, dtype=float32)),\n",
       " (Array(0.00552423, dtype=float64),\n",
       "  Array(0.00748995, dtype=float64),\n",
       "  Array(0.00061389, dtype=float64),\n",
       "  Array(0.00748981, dtype=float64),\n",
       "  Array(0.00552422, dtype=float64),\n",
       "  Array(7.2011147e-13, dtype=float64)),\n",
       " (Array(4.28741647e-08, dtype=float64), Array(3.31068861e-07, dtype=float64)),\n",
       " Array(0., dtype=float64),\n",
       " Array(7.99967731e-15, dtype=float64),\n",
       " Array(1.78052018e-13, dtype=float64))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each parameter, the mean absolute different in gradient values\n",
    "tree_map(lambda x,y: jnp.abs(jnp.mean(x-y)), unrolled_grads, implicit_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dfd9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
